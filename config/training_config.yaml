# Pro Roofing AI Training Configuration

# Model Configuration
model:
  name: "meta-llama/Llama-2-7b-chat-hf"  # Base model to fine-tune
  cache_dir: "./models/base"
  torch_dtype: "float16"  # Use float16 for memory efficiency
  device_map: "auto"
  load_in_8bit: false
  load_in_4bit: true  # Enable 4-bit quantization for memory efficiency

# LoRA Configuration (Parameter Efficient Fine-tuning)
lora:
  r: 16  # Rank of the adaptation
  alpha: 32  # Scaling parameter
  dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Training Parameters
training:
  output_dir: "./models/checkpoints"
  overwrite_output_dir: true
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  
  # Learning Rate
  learning_rate: 2e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.01
  
  # Optimization
  optim: "paged_adamw_32bit"
  max_grad_norm: 1.0
  
  # Logging and Saving
  logging_steps: 10
  save_steps: 500
  eval_steps: 100
  save_total_limit: 3
  evaluation_strategy: "steps"
  
  # Data
  max_seq_length: 2048
  dataloader_num_workers: 4
  remove_unused_columns: false
  
  # Memory and Performance
  fp16: false
  bf16: true  # Use bfloat16 if available
  tf32: true
  dataloader_pin_memory: true
  group_by_length: true

# Data Configuration
data:
  train_file: "./data/processed/ultimate_roofing_training.jsonl"
  validation_file: "./data/processed/validation_set.jsonl"
  text_column: "text"
  max_train_samples: null  # Use all samples
  max_eval_samples: 1000
  
  # Data Processing
  preprocessing_num_workers: 4
  keep_linebreaks: true
  
  # Validation Split
  validation_split_percentage: 10
  seed: 42

# WandB Configuration
wandb:
  project: "pro-roofing-ai"
  name: "roofing-llama2-7b-lora"
  tags:
    - "roofing"
    - "llama2"
    - "lora"
    - "fine-tuning"
  notes: "Fine-tuning Llama-2-7b on roofing industry data"
  
# Early Stopping
early_stopping:
  patience: 3
  threshold: 0.01
  metric: "eval_loss"
  mode: "min"

# Hardware Configuration
hardware:
  use_cuda: true
  cuda_devices: "auto"
  mixed_precision: "bf16"
  
  # Multi-GPU settings
  world_size: 1
  local_rank: -1
  ddp_find_unused_parameters: false

# Inference Configuration
inference:
  do_sample: true
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  max_new_tokens: 512
  pad_token_id: null  # Will be set automatically

# Lambda Labs Specific
lambda_labs:
  instance_type: "gpu_1x_a100_sxm4"
  region: "us-east-1"
  auto_terminate: true
  max_runtime_hours: 12

# Checkpointing
checkpoint:
  resume_from_checkpoint: null
  save_safetensors: true
  save_on_each_node: false
  
# Final Model Export
export:
  merge_and_unload: true
  output_dir: "./models/final"
  push_to_hub: false
  hub_model_id: null
  
# Monitoring
monitoring:
  track_memory: true
  log_gpu_stats: true
  profile_memory: false
  
# Advanced Settings
advanced:
  use_reentrant: false  # For gradient checkpointing
  torch_compile: false  # Experimental PyTorch compilation
  trust_remote_code: true
  use_fast_tokenizer: true